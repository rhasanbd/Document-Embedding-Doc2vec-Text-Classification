{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Embedding\n",
    "\n",
    "The two key questions in Natural Language Processing (NLP) are:\n",
    "- How do we represent words numerically in a text by maintaining its linguistic regularities and patterns?\n",
    "- How do we represent documents numerically by retaining the semantics of the words and word orders in the document?\n",
    "\n",
    "The first question can be addressed by creating word embedding using a technique named Word2vec. It was proposed in 2013 by a team of researchers led by Google’s Tomas Mikolov. It is described in the following notebook:\n",
    "https://github.com/rhasanbd/Word-Embedding-by-Word2vec\n",
    "\n",
    "The second question is addrssed by the same research group (Quoc Le and Tomas Mikolov) in 1994. They proposed a **Paragraph Vector** framework implemented by the **Doc2Vec** algorithm. It is an unsupervised framework to learn continuous distributed vector representations for pieces of texts.\n",
    "https://cs.stanford.edu/~quocle/paragraph_vector.pdf\n",
    "\n",
    "\n",
    "## Doc2vec: Main Idea\n",
    "\n",
    "The Doc2vec model is created by slightly augmenting the Word2vec model. We only need to add an extra vector.\n",
    "\n",
    "More formally it's a small extension to the Continuous Bag-of-Words (CBOW) Word2vec model. Instead of using just words to predict the next word, it also adds another feature vector, which is document-unique.\n",
    "\n",
    "There are two implementations Doc2vec model:\n",
    "- Distributed Memory (DM)\n",
    "- Distributed Bag of Words (DBOW)\n",
    "\n",
    "\n",
    "        Let's briefly describe the DM algorithm.\n",
    "\n",
    "\n",
    "## Distributed Memory (DM) Algorithm\n",
    "\n",
    "Consider the CBOW model as shown below. Three words (“the,” “cat,” and “sat”) are used to predict the fourth word (“on”). The input words are mapped to columns of the matrix W to predict the output word.\n",
    "\n",
    "<img src=\"http://engineering.unl.edu/images/uploads/WordVector.png\" width=400, height=150>\n",
    "\n",
    "\n",
    "Here the word vectors are asked to contribute to a prediction task about the next word in the sentence. Despite the fact that the word vectors are initialized randomly, they can eventually capture semantics as an indirect result of the prediction task. \n",
    "\n",
    "In the DM algorithm, when training the word vectors W, the document vector D is trained as well, and in the end of training, it holds a numeric representation of the document.\n",
    "\n",
    "See the DM Paragraph Vector framework below. Every paragraph is mapped to a unique vector, represented by a column in matrix D and every word is also mapped to a unique vector, represented by a column in matrix W . The paragraph vector and word vectors are averaged or concatenated to predict the next word in a context. \n",
    "\n",
    "\n",
    "<img src=\"http://engineering.unl.edu/images/uploads/ParagraphVector.png\" width=400, height=150>\n",
    "\n",
    "This model is called Distributed Memory version of Paragraph Vector (PV-DM) because it acts as a memory that remembers what is missing from the current context, or as the topic of the paragraph. \n",
    "\n",
    "While the word vectors represent the concept of a word, the document vector intends to represent the concept of a document.\n",
    "\n",
    "\n",
    "\n",
    "## Experiment\n",
    "\n",
    "We perform a text classification task to investigate the performance of the Doc2vec (PV-DM algorithm) technique. We use an email dataset for binary classification (spam and ham).\n",
    "\n",
    "URL: https://www.kaggle.com/karthickveerakumar/spam-filter\n",
    "\n",
    "The dataset consists of labeled emails belonging to both ham (label = 0) and spam (label = 1).\n",
    "\n",
    "Previously we used Bag-of-Words based technique with Naive Bayes classifiers for classification on this dataset. We use it as a benchmark for comparison:\n",
    "https://github.com/rhasanbd/Naive-Bayes-Algorithms-Foray-Into-Text-Classification/blob/master/Naive%20Bayes-Text%20Classification-Multinomial-Bernoulli.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "import seaborn as sns\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "\n",
    "Load data from the CSV file as Pandas DataFrame object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/hasan/datasets/emails.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=blue> 1. Exploratory Data Analysis</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Check of the Data\n",
    "\n",
    "Let’s take a look at the top five rows using the DataFrame’s head() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Subject: naturally irresistible your corporate...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Subject: the stock trading gunslinger  fanny i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Subject: unbelievable new homes made easy  im ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Subject: 4 color printing special  request add...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Subject: do not have money , get software cds ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  spam\n",
       "0  Subject: naturally irresistible your corporate...     1\n",
       "1  Subject: the stock trading gunslinger  fanny i...     1\n",
       "2  Subject: unbelievable new homes made easy  im ...     1\n",
       "3  Subject: 4 color printing special  request add...     1\n",
       "4  Subject: do not have money , get software cds ...     1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5728 entries, 0 to 5727\n",
      "Data columns (total 2 columns):\n",
      "text    5728 non-null object\n",
      "spam    5728 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 89.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of the data:  (5728, 2)\n",
      "\n",
      "No. of Rows: 5728\n",
      "No. of Columns: 2\n"
     ]
    }
   ],
   "source": [
    "print(\"Dimension of the data: \", df.shape)\n",
    "\n",
    "no_of_rows = df.shape[0]\n",
    "no_of_columns = df.shape[1]\n",
    "\n",
    "print(\"\\nNo. of Rows: %d\" % no_of_rows)\n",
    "print(\"No. of Columns: %d\" % no_of_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1368</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      text\n",
       "spam      \n",
       "0     4360\n",
       "1     1368"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('spam').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of the Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "label_counts = df.spam.value_counts()\n",
    "plt.figure(figsize = (6,4))\n",
    "sns.barplot(label_counts.index, label_counts.values, alpha = 0.9)\n",
    "\n",
    "plt.xticks(rotation = 'vertical')\n",
    "plt.xlabel('Spam', fontsize =12)\n",
    "plt.ylabel('Counts', fontsize = 12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Document Corpus\n",
    "\n",
    "Get the \"text\" column of the DataFrame object as the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Documents (emails) in the Corpus:  5728\n"
     ]
    }
   ],
   "source": [
    "corpus = df['text']\n",
    "\n",
    "print(\"Number of Documents (emails) in the Corpus: \", len(corpus))\n",
    "\n",
    "#corpus[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Embedding using Doc2Vec\n",
    "\n",
    "The Gensim Doc2vector model requires the corpus as a list of lists containing tokenized words per document and document tags, as shown below.\n",
    "\n",
    "    ['feynman', 'teach', 'physic'], [0] \n",
    "    ['physic', 'cool'], [1]\n",
    "\n",
    "\n",
    "Thus, before we perform document embedding, we need to **pre-process** the data by implementing the following steps:\n",
    "- Create a list of documents\n",
    "- Convert it to a list of lists containing tokenized words\n",
    "- Create tagged documents with tokenized words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process the Data\n",
    "\n",
    "\n",
    "We pre-process the data as follows. \n",
    "\n",
    "- Convert to lowercase \n",
    "- Tokenize (split the documents into tokens or words)\n",
    "- Remove numbers, but not words that contain numbers\n",
    "- Remove words that are only one character\n",
    "- Lemmatize the tokens/words\n",
    "\n",
    "\n",
    "### Tokenization\n",
    "\n",
    "We tokenize the text using a regular expression tokenizer from NLTK. We remove numeric tokens and tokens that are only a single character, as they don’t tend to be useful, and the dataset contains a lot of them.\n",
    "\n",
    "\n",
    "The NLTK Regular-Expression Tokenizer class \"RegexpTokenizer\" splits a string into substrings using a regular expression. We use the regular expression \"\\w+\" to matche token of words. \n",
    "\n",
    "- \"\\w\" instructs to escape words.\n",
    "- \"+\" is a quantifier that means 1 or more. \n",
    "\n",
    "See the following two links for a list of regular expressions and NLTK tokenize module.\n",
    "https://github.com/tartley/python-regex-cheatsheet/blob/master/cheatsheet.rst\n",
    "https://www.nltk.org/api/nltk.tokenize.html\n",
    "\n",
    "\n",
    "## Function to Convert the 2D Document Array into a 2D Array of Tokenized Documents\n",
    "\n",
    "- Note that we don't remove numbers and words with one characters for acieving better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for convert a list of sentences to a list of lists containing tokenized words\n",
    "def docs_preprocessor(docs):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+') # Tokenize the words.\n",
    "    \n",
    "    for idx in range(len(docs)):\n",
    "        docs[idx] = docs[idx].lower()  # Convert to lowercase.\n",
    "        docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n",
    "\n",
    "    # Remove numbers, but not words that contain numbers.\n",
    "    docs = [[token for token in doc if not token.isdigit()] for doc in docs]\n",
    "    \n",
    "    # Remove words that are only one character.\n",
    "    docs = [[token for token in doc if len(token) > 3] for doc in docs]\n",
    "    \n",
    "    # Lemmatize all words in documents.\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]\n",
    "  \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the 2D Document Array into a 2D Array of Tokenized Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hasan/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/Users/hasan/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5728\n",
      "CPU times: user 8min 46s, sys: 1.1 s, total: 8min 47s\n",
      "Wall time: 8min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Convert a list of sentences to a list of lists containing tokenized words\n",
    "docs = docs_preprocessor(corpus)\n",
    "print(len(docs))\n",
    "\n",
    "\n",
    "# Store the data locally\n",
    "pickle.dump(docs, open(\"/Users/hasan/datasets_output/tokenized_spam_docs_large1_doc2vec.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Bigrams/Trigrams:\n",
    "\n",
    "\n",
    "When topics are very similar, we may **use phrases** rather than single/individual words to distinguis each topic. \n",
    "\n",
    "Thus, we compute both bigrams (and may be trigrams). \n",
    "- Depending on the dataset it may not be necessary to create trigrams.\n",
    "\n",
    "Note that we only keep the **frequent** phrases (bigrams/trigrams).\n",
    "\n",
    "#### Bigrams\n",
    "Bigrams are sets of two adjacent words. Using bigrams we can get phrases like “machine_learning” in our output (spaces are replaced with underscores). Without bigrams we would only get “machine” and “learning”.\n",
    "\n",
    "- Note: we don't extract bigrams/trigrams. They don't increase accuracy of classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import Phrases\n",
    "\n",
    "# # Add bigrams and trigrams to docs (only ones that appear 10 times or more).\n",
    "# bigram = Phrases(docs, min_count=10)\n",
    "# trigram = Phrases(bigram[docs])\n",
    "\n",
    "# for idx in range(len(docs)):\n",
    "#     for token in bigram[docs[idx]]:\n",
    "#         if '_' in token:\n",
    "#             # Token is a bigram, add to document.\n",
    "#             docs[idx].append(token)\n",
    "#     for token in trigram[docs[idx]]:\n",
    "#         if '_' in token:\n",
    "#             # Token is a bigram, add to document.\n",
    "#             docs[idx].append(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Tagged Documents\n",
    "\n",
    "For training the Doc2vec model, we need to create tagged documents.\n",
    "\n",
    "A single document, made up of words and tags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the list of lists containing tokenized words\n",
    "#docs = pickle.load( open(\"/Users/hasan/datasets_output/tokenized_spam_docs_doc2vec.p\", \"rb\" ) )\n",
    "docs = pickle.load( open(\"/Users/hasan/datasets_output/tokenized_spam_docs_large1_doc2vec.p\", \"rb\" ) )\n",
    "#print(docs[0])\n",
    "\n",
    "# Create Tagged documents\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(docs)]\n",
    "#print(documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Doc2vec Model\n",
    "\n",
    "We use the gensim.models.Doc2Vec class.\n",
    "\n",
    "        class gensim.models.doc2vec.Doc2Vec(documents=None, corpus_file=None, dm_mean=None, dm=1, dbow_words=0, dm_concat=0, dm_tag_count=1, docvecs=None, docvecs_mapfile=None, comment=None, trim_rule=None, callbacks=(), **kwargs)\n",
    "        \n",
    "We need to set the parameters of the Doc2Vec object carefully. The full list of the parameters are given:\n",
    "\n",
    "https://radimrehurek.com/gensim/models/doc2vec.html\n",
    "\n",
    "Below we discuss the settings of some of the key parameters.\n",
    "\n",
    "- documents (iterable of list of TaggedDocument, optional)\n",
    "\n",
    "        Input corpus, can be simply a list of elements.\n",
    "\n",
    "\n",
    "- dm ({1,0}, optional)\n",
    "\n",
    "        Defines the training algorithm. If dm=1, ‘distributed memory’ (PV-DM) is used. Otherwise, distributed bag of words (PV-DBOW) is employed.\n",
    "\n",
    "- vector_size (int, optional)\n",
    "\n",
    "        Dimensionality of the feature vectors.\n",
    "\n",
    "- window (int, optional)\n",
    "\n",
    "        The maximum distance between the current and predicted word within a sentence.\n",
    "\n",
    "- alpha (float, optional)\n",
    "\n",
    "        The initial learning rate.\n",
    "\n",
    "- min_alpha (float, optional)\n",
    "\n",
    "        Learning rate will linearly drop to min_alpha as training progresses.\n",
    "\n",
    "- seed (int, optional)\n",
    "\n",
    "        Seed for the random number generator. Initial vectors for each word are seeded with a hash of the concatenation of word + str(seed). \n",
    "        \n",
    "\n",
    "- min_count (int, optional)\n",
    "\n",
    "        Ignores all words with total frequency lower than this.\n",
    "\n",
    "\n",
    "- sample (float, optional)\n",
    "\n",
    "        The threshold for configuring which higher-frequency words are randomly downsampled, useful range is (0, 1e-5).\n",
    "\n",
    "\n",
    "- workers (int, optional)\n",
    "\n",
    "        Use these many worker threads to train the model (=faster training with multicore machines).\n",
    "\n",
    "\n",
    "- epochs (int, optional)\n",
    "\n",
    "        Number of iterations (epochs) over the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Doc2vec Model and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
      "Wall time: 8.34 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Set training parameters\n",
    "doc_vector_length = 300       # Dimension of the document vector\n",
    "window_size = 2               # We set it 2 as the sentences weren't too long\n",
    "epochs = 600                  # Number of iterations (epochs) over the corpus\n",
    "min_count = 1                 # Ignores all words with total frequency lower than min_count\n",
    "workers = 4                   # Number of worker threads to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20min 25s, sys: 55.1 s, total: 21min 20s\n",
      "Wall time: 7min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Create the Doc2vec model using gensim (If dm=1, ‘distributed memory’ (DM) algorithm is used)\n",
    "model = Doc2Vec(vector_size=doc_vector_length, dm=1, window=window_size, min_count=min_count, \n",
    "                workers=workers, sample=0.01, epochs=epochs)\n",
    "# Create vocabulary\n",
    "model.build_vocab(documents)\n",
    "\n",
    "# Train the model\n",
    "model.train(documents, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('d2v_model_email_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc2vec model\n",
    "model = Doc2Vec.load('d2v_model_email_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size:  28820\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary Size: \", len(model.wv.vocab))\n",
    "\n",
    "# print(\"Vocabulary: \")\n",
    "# for word in model.wv.vocab:\n",
    "#     print(\" \", word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data Matrix X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5728, 300)\n"
     ]
    }
   ],
   "source": [
    "noOfDocuments = len(documents)\n",
    "\n",
    "X = np.empty([noOfDocuments, doc_vector_length])\n",
    "\n",
    "for i in range(noOfDocuments):\n",
    "    X[i] = model.docvecs[i].reshape(1, -1)\n",
    "    \n",
    "print(X.shape)\n",
    "#print(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Target (1D Vector y)\n",
    "\n",
    "Get the \"spam\" column of the DataFrame object as the target vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5728,)\n"
     ]
    }
   ],
   "source": [
    "y = df['spam'] # 1D targer vector\n",
    "\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Train and Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification \n",
    "\n",
    "After embedding the text by using the Doc2vec model, we perform classification.\n",
    "\n",
    "We use four classification models:\n",
    "- Linear Support Vector Machine\n",
    "- Non-Linear Support Vector Machine (Gaussian RBF Kernel)\n",
    "- Logistic Regression\n",
    "- Gaussian Naive Bayes Classifier\n",
    "\n",
    "\n",
    "#### The optimal performance is achieved by the Linear SVM model. This is due to the linear separability of the data.\n",
    "\n",
    "\n",
    "## Linear Support Vector Machine\n",
    "\n",
    "The the LinearSVC class uses the Coordinate Descent approach. \n",
    "\n",
    "### Hyperparameter Settings:\n",
    "- The \"loss\" hyperparameter should be set to \"hinge\".\n",
    "- The hyperparameter \"C\" controls the penalty for the error (margin violation). It should be selected via grid search. We will investigate its effect shortly.\n",
    "- Finally, for better performance we should set the \"dual\" hyperparameter to False, unless there are more features than training instances.\n",
    "\n",
    "      Before applying SVM we need to standardize the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hasan/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "scaled_svm_clf = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"clf\", LinearSVC(C=1, loss=\"hinge\",  max_iter=5000, random_state=42))\n",
    "    ])\n",
    "\n",
    "\n",
    "scaled_svm_clf.fit(X_train, y_train)\n",
    "\n",
    "y_train_predicted = scaled_svm_clf.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy:\n",
      "0.9450261780104712\n",
      "\n",
      "Test Confusion Matrix:\n",
      "[[817  39]\n",
      " [ 24 266]]\n",
      "\n",
      "Test Precision = 0.872131\n",
      "Test Recall = 0.917241\n",
      "Test F1 Score = 0.894118\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Ham       0.97      0.95      0.96       856\n",
      "        Spam       0.87      0.92      0.89       290\n",
      "\n",
      "    accuracy                           0.95      1146\n",
      "   macro avg       0.92      0.94      0.93      1146\n",
      "weighted avg       0.95      0.95      0.95      1146\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test_predicted = scaled_svm_clf.predict(X_test)\n",
    "\n",
    "\n",
    "print(\"\\nTest Accuracy:\")\n",
    "print(scaled_svm_clf.score(X_test, y_test))\n",
    "\n",
    "\n",
    "print(\"\\nTest Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_predicted))\n",
    "\n",
    "precision_test = precision_score(y_test, y_test_predicted) \n",
    "print(\"\\nTest Precision = %f\" % precision_test)\n",
    "\n",
    "recall_test = recall_score(y_test, y_test_predicted)\n",
    "print(\"Test Recall = %f\" % recall_test)\n",
    "\n",
    "\n",
    "f1_test = f1_score(y_test, y_test_predicted)\n",
    "print(\"Test F1 Score = %f\" % f1_test)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_predicted, target_names = [\"Ham\", \"Spam\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-linear SVM\n",
    "\n",
    "We experiment with SVM Gaussian Radial Basis Function or RBF model. \n",
    "\n",
    "\n",
    "First, we find the optimal hyperparameters via grid search. Then, train a Gaussian RBF SVM model using the optimal hyperparameters.\n",
    "\n",
    "However its performance is not as good as that of the linear SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:   13.2s\n",
      "[Parallel(n_jobs=-1)]: Done  80 out of  80 | elapsed:  1.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.932254\n",
      "Optimal Hyperparameter Values:  {'C': 100, 'gamma': 0.0001}\n",
      "\n",
      "\n",
      "CPU times: user 1.72 s, sys: 218 ms, total: 1.93 s\n",
      "Wall time: 1min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# The param_grid tells Scikit-Learn to evaluate all combinations of the hyperparameter values\n",
    "param_grid = {'gamma': [0.00001, 0.0001, 0.01, 0.1], 'C': [1000, 500, 100, 10]}\n",
    "\n",
    "svm_rbf_clf = SVC(kernel=\"rbf\")\n",
    "\n",
    "svm_cv = GridSearchCV(svm_rbf_clf, param_grid, scoring='f1', cv=5, verbose=3, n_jobs=-1)\n",
    "svm_cv.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "params_optimal_svm = svm_cv.best_params_\n",
    "\n",
    "print(\"Best Score: %f\" % svm_cv.best_score_)\n",
    "print(\"Optimal Hyperparameter Values: \", params_optimal_svm)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Linear SVM: Train the Optimal Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy:\n",
      "0.9703315881326352\n",
      "\n",
      "Test Confusion Matrix:\n",
      "[[843  13]\n",
      " [ 21 269]]\n",
      "\n",
      "Test Precision = 0.953901\n",
      "Test Recall = 0.927586\n",
      "Test F1 Score = 0.940559\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Ham       0.98      0.98      0.98       856\n",
      "        Spam       0.95      0.93      0.94       290\n",
      "\n",
      "    accuracy                           0.97      1146\n",
      "   macro avg       0.96      0.96      0.96      1146\n",
      "weighted avg       0.97      0.97      0.97      1146\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm_rbf = SVC(**params_optimal_svm)\n",
    "\n",
    "svm_rbf.fit(X_train, y_train)\n",
    "\n",
    "y_test_predicted = svm_rbf.predict(X_test)\n",
    "\n",
    "\n",
    "print(\"\\nTest Accuracy:\")\n",
    "print(svm_rbf.score(X_test, y_test))\n",
    "\n",
    "\n",
    "print(\"\\nTest Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_predicted))\n",
    "\n",
    "precision_test = precision_score(y_test, y_test_predicted) \n",
    "print(\"\\nTest Precision = %f\" % precision_test)\n",
    "\n",
    "recall_test = recall_score(y_test, y_test_predicted)\n",
    "print(\"Test Recall = %f\" % recall_test)\n",
    "\n",
    "\n",
    "f1_test = f1_score(y_test, y_test_predicted)\n",
    "print(\"Test F1 Score = %f\" % f1_test)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_predicted, target_names = [\"Ham\", \"Spam\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "First, we perform hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# param_grid = {'solver': ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga'], \n",
    "#               'tol': [1e-3, 1e-4], 'max_iter':[3000, 5000, 10000],'C': [0.001, 0.1, 0.5, 1, 10]}\n",
    "\n",
    "# lg_reg = LogisticRegression()\n",
    "\n",
    "# lg_reg_cv = GridSearchCV(lg_reg, param_grid, scoring='accuracy', cv=3, verbose=1, n_jobs=-1)\n",
    "# lg_reg_cv.fit(X_train, y_train)\n",
    "\n",
    "# params_optimal = lg_reg_cv.best_params_\n",
    "\n",
    "# print(\"Best Score (accuracy): %f\" % lg_reg_cv.best_score_)\n",
    "# print(\"Optimal Hyperparameter Values: \", params_optimal)\n",
    "# print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression: Train the Optimal Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lg_reg_clf = LogisticRegression(**params_optimal)\n",
    "\n",
    "# lg_reg_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test_predicted = lg_reg_clf.predict(X_test)\n",
    "\n",
    "\n",
    "# print(\"\\nTest Accuracy:\")\n",
    "# print(lg_reg_clf.score(X_test, y_test))\n",
    "\n",
    "\n",
    "# print(\"\\nTest Confusion Matrix:\")\n",
    "# print(confusion_matrix(y_test, y_test_predicted))\n",
    "\n",
    "# precision_test = precision_score(y_test, y_test_predicted) \n",
    "# print(\"\\nTest Precision = %f\" % precision_test)\n",
    "\n",
    "# recall_test = recall_score(y_test, y_test_predicted)\n",
    "# print(\"Test Recall = %f\" % recall_test)\n",
    "\n",
    "\n",
    "# f1_test = f1_score(y_test, y_test_predicted)\n",
    "# print(\"Test F1 Score = %f\" % f1_test)\n",
    "\n",
    "# print(\"\\nClassification Report:\")\n",
    "# print(classification_report(y_test, y_test_predicted, target_names = [\"Ham\", \"Spam\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes (Gaussian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussianNB_clf = GaussianNB(var_smoothing=0.01).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy:\n",
      "0.7164048865619547\n",
      "\n",
      "Test Confusion Matrix:\n",
      "[[625 231]\n",
      " [ 94 196]]\n",
      "\n",
      "Test Precision = 0.459016\n",
      "Test Recall = 0.675862\n",
      "Test F1 Score = 0.546722\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Ham       0.87      0.73      0.79       856\n",
      "        Spam       0.46      0.68      0.55       290\n",
      "\n",
      "    accuracy                           0.72      1146\n",
      "   macro avg       0.66      0.70      0.67      1146\n",
      "weighted avg       0.77      0.72      0.73      1146\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test_predicted = gaussianNB_clf.predict(X_test)\n",
    "\n",
    "\n",
    "print(\"\\nTest Accuracy:\")\n",
    "print(gaussianNB_clf.score(X_test, y_test))\n",
    "\n",
    "\n",
    "print(\"\\nTest Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_predicted))\n",
    "\n",
    "precision_test = precision_score(y_test, y_test_predicted) \n",
    "print(\"\\nTest Precision = %f\" % precision_test)\n",
    "\n",
    "recall_test = recall_score(y_test, y_test_predicted)\n",
    "print(\"Test Recall = %f\" % recall_test)\n",
    "\n",
    "\n",
    "f1_test = f1_score(y_test, y_test_predicted)\n",
    "print(\"Test F1 Score = %f\" % f1_test)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_predicted, target_names = [\"Ham\", \"Spam\"]))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
